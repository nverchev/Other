{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chamfer distance for Pytorch: comparison",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nverchev/other_Python_projects/blob/main/Chamfer_distance_for_Pytorch_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What is the best implementation of the Chamfer Distance?\n",
        "\n",
        "We compare a simple implementation with other two coming from two libraries.\n",
        "- https://github.com/otaheri/chamfer_distance\n",
        "- https://pypi.org/project/chamferdist/1.0.0/#description\n",
        "\n",
        "These implementatations are compatible with torch.autograd, and we test them using GPU power.\n",
        "\n",
        "I suggest to run the experiment different times to account for some possible overhead.\n",
        "\n",
        "---\n",
        "# Update\n",
        "In terms of effiency AND scalability the better option seems to be [KeOps](https://www.kernel-operations.io/keops/index.html).\n",
        "Unfortunately the min() operation is not supported for backprop.\n",
        "The provided code uses the index to recalculate the distances in torch.\n"
      ],
      "metadata": {
        "id": "_x5EAhMS0xKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#First implementation \n",
        "!pip install git+'https://github.com/otaheri/chamfer_distance'\n",
        "\n",
        "#Second implementation\n",
        "!pip install chamferdist\n"
      ],
      "metadata": {
        "id": "zFeIlNYZ17ZP",
        "outputId": "a11101c9-6945-4db1-e75f-2d487a0afd23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/otaheri/chamfer_distance\n",
            "  Cloning https://github.com/otaheri/chamfer_distance to /tmp/pip-req-build-pli7hdsh\n",
            "  Running command git clone -q https://github.com/otaheri/chamfer_distance /tmp/pip-req-build-pli7hdsh\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from chamfer-distance==0.1) (1.11.0+cu113)\n",
            "Collecting Ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->chamfer-distance==0.1) (4.2.0)\n",
            "Building wheels for collected packages: chamfer-distance\n",
            "  Building wheel for chamfer-distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chamfer-distance: filename=chamfer_distance-0.1-py3-none-any.whl size=5653 sha256=84e16ae4449269ca5b8b38921a724bdbaec36a87e87212e1e92b41222c416958\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8x5db3zx/wheels/2a/c5/7c/395771526a57f81590f5b9e2be57f219f834d894e10b1cd993\n",
            "Successfully built chamfer-distance\n",
            "Installing collected packages: Ninja, chamfer-distance\n",
            "Successfully installed Ninja-1.10.2.3 chamfer-distance-0.1\n",
            "Collecting chamferdist\n",
            "  Downloading chamferdist-1.0.0.tar.gz (16 kB)\n",
            "Building wheels for collected packages: chamferdist\n",
            "  Building wheel for chamferdist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chamferdist: filename=chamferdist-1.0.0-cp37-cp37m-linux_x86_64.whl size=5836276 sha256=7817a636b732c8407dbc2a7874a0e19435ebf8f8ee8aa44d2cf80d1fed00346e\n",
            "  Stored in directory: /root/.cache/pip/wheels/28/bb/d1/c789ecd6835e466e813f6e2c5e23bb1bbb2248e84586ba82d2\n",
            "Successfully built chamferdist\n",
            "Installing collected packages: chamferdist\n",
            "Successfully installed chamferdist-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pykeops > install.log"
      ],
      "metadata": {
        "id": "v_eJ7gxpZKrJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "U6oFrClH0f-Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "pc1 = torch.rand([100,1000,3]).to()\n",
        "pc2 = torch.rand([100,100,3]).to()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chamfer_distance import ChamferDistance\n",
        "chamdist = ChamferDistance()\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start.record()\n",
        "d1,d2,_,_ = chamdist(pc1,pc2) # dist forward, dist reverse\n",
        "d = d1.sum(axis=1).mean() + d2.sum(axis=1).mean() # batchmean\n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "print(\"First implementation:\")\n",
        "print(\"Time (ms): \", start.elapsed_time(end))\n",
        "print(\"Result: \", d)"
      ],
      "metadata": {
        "id": "aRiecm8Y2KrY",
        "outputId": "e702beb9-46ad-432c-e728-36347d88e230",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First implementation:\n",
            "Time (ms):  199.6451873779297\n",
            "Result:  tensor(19.5153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chamferdist import ChamferDistance\n",
        "chamdist = ChamferDistance()\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start.record()\n",
        "d = chamdist(pc1,pc2, bidirectional=True, reduction=\"mean\") \n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "print(\"Second implementation:\")\n",
        "print(\"Time (ms): \", start.elapsed_time(end))\n",
        "print(\"Result: \", d)"
      ],
      "metadata": {
        "id": "Kx7D1AOO2LE7",
        "outputId": "6a5fc077-72f6-4f06-b637-a6db48c1ee88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Second implementation:\n",
            "Time (ms):  183.89068603515625\n",
            "Result:  tensor(19.5153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def square_distance(t1, t2):\n",
        "    t2 = t2.permute(0, 2, 1)\n",
        "    dist = -2 * torch.matmul(t1, t2)\n",
        "    dist += torch.sum(t1 ** 2, 2, keepdim=True)\n",
        "    dist += torch.sum(t2 ** 2, 1, keepdim=True)\n",
        "    return dist\n",
        "    \n",
        "# Chamfer Distance\n",
        "def chamdist(t1, t2): \n",
        "    dist = square_distance(t1, t2)\n",
        "    # forward + reverse\n",
        "    return torch.min(dist, axis = 2)[0].mean(0).sum()\\\n",
        "         + torch.min(dist, axis = 1)[0].mean(0).sum()\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start.record()\n",
        "d = chamdist(pc1, pc2)\n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "print(\"Simple implementation:\")\n",
        "print(\"Time: \", start.elapsed_time(end))\n",
        "print(\"Result: \", d)"
      ],
      "metadata": {
        "id": "7JA3VF7x2LoF",
        "outputId": "0ee22556-a67b-4746-a401-e57d0ba716fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple implementation:\n",
            "Time:  69.26172637939453\n",
            "Result:  tensor(19.5153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pykeops.torch import LazyTensor\n",
        "def square_distance(t1, t2):\n",
        "    t1 = LazyTensor(t1[:, :, None, :])\n",
        "    t2 = LazyTensor(t2[:, None,:, :])\n",
        "    dist = ((t1 - t2) ** 2).sum(-1)\n",
        "    return dist\n",
        "    \n",
        "# Chamfer Distance\n",
        "def chamdist(t1, t2): \n",
        "    dist = square_distance(t1, t2)\n",
        "    # The following code is currently not supported for backprop\n",
        "    # return dist.min(axis = 2).mean(0).sum()\\\n",
        "    #      + dist.min(axis = 1).mean(0).sum()\n",
        "\n",
        "    # We use the retrieved index on torch\n",
        "    idx1 = dist.argmin(axis = 1).expand(-1, -1, 3)\n",
        "    m1 = t1.gather(1, idx1)\n",
        "    s1 = ((t2 - m1) ** 2).mean(0).sum()\n",
        "    idx2 = dist.argmin(axis = 2).expand(-1, -1, 3)\n",
        "    m2 = t2.gather(1, idx2)\n",
        "    s2 = ((t1 - m2) ** 2).mean(0).sum()\n",
        "    # forward + reverse\n",
        "    return s1 + s2\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start.record()\n",
        "d = chamdist(pc1, pc2)\n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "print(\"Simple implementation:\")\n",
        "print(\"Time: \", start.elapsed_time(end))\n",
        "print(\"Result: \", d)"
      ],
      "metadata": {
        "id": "onO93iEvZUHW",
        "outputId": "146dc8af-2db5-4433-b57e-b000520a944b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple implementation:\n",
            "Time:  5.831488132476807\n",
            "Result:  tensor(19.5153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "Surprisingly, the simple implementation in torch is quicker than the ones from the two libraries. KeOps is by far the best backend, even without the backprop support for min().\n",
        "Hope this could be useful to anybody starting with PCD.\n"
      ],
      "metadata": {
        "id": "SP0qTMfE3wcG"
      }
    }
  ]
}